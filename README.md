# Day 1 â€“ UK Retail Sales ETL Pipeline

**Cleaning messy multi-source sales data into a trustworthy database**

[![Python](https://img.shields.io/badge/Python-3.13-blue.svg)](https://www.python.org/)
[![pandas](https://img.shields.io/badge/pandas-latest-green.svg)](https://pandas.pydata.org/)
[![SQLite](https://img.shields.io/badge/SQLite-3-orange.svg)](https://www.sqlite.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

---

## ğŸ“‹ Problem Statement

A mid-sized UK fashion retailer (think high-street chain or growing e-commerce brand) receives daily sales data from two very different systems:

- **In-store POS system** â†’ CSV export
- **Online Shopify store** â†’ CSV export

**The files are messy:**

- Different column names (`Qty` vs `quantity_sold`, `Price` vs `unit_price`, etc.)
- Mixed date formats (`2026-01-21`, `01-21-2026`, `21/01/2026`)
- Missing values (especially quantities and store locations)
- Duplicates
- Inconsistent / free-text product categories (`Apparel`, `apparel `, `clothing typo`)

**Business Impact:**

Finance and marketing teams need **reliable daily sales figures** by:
- Product category
- Store location  
- Sales channel (in-store vs online)

Without clean data, they cannot accurately track performance, plan stock, or run promotions.

---

## ğŸ¯ Business Goal

Build a **repeatable, trustworthy ETL process** that:

âœ… Takes raw, messy CSVs every day  
âœ… Cleans and standardizes the data  
âœ… Loads it into a queryable database  
âœ… Enables analysts to answer real questions using SQL or BI tools (Power BI, Tableau, Excel)

---

## ğŸ”§ What I Built

A Python + pandas ETL pipeline that:

### 1. **Extract**
- Reads two messy CSV files from different sources

### 2. **Transform**
- Standardizes column names across sources
- Removes duplicates
- Cleans categories (lowercase, strip, remove typos)
- **Custom date parsing** to handle mixed UK/US/ISO formats
  - *This was the hardest part â€” automatic parsing failed on ~95% of rows*
  - Built custom logic â†’ **0% failure rate**
- Handles missing values (quantities â†’ 0, locations â†’ 'Unknown')
- Adds calculated revenue check & date dimension columns

### 3. **Load**
- Saves clean data into **SQLite** database

### 4. **Consume**
- Produces business-ready SQL queries

---

## ğŸ“Š Key Results

### Revenue & Transaction Analysis

| Channel  | Total Revenue | Transactions | Avg Transaction Value |
|----------|--------------|--------------|----------------------|
| In-store | Â£31,343.10   | 103          | Â£304.30             |
| Shopify  | Â£23,291.46   | 100          | Â£232.91             |

**Insight:** In-store customers spend **31% more** per transaction â€” signals opportunity to enhance in-store experience and optimize online basket size.

### Top Revenue Categories

| Category    | Total Revenue | Units Sold | Unique Products |
|-------------|--------------|------------|-----------------|
| Apparel     | Â£18,750.17   | 342        | 8               |
| Clothing    | Â£16,734.17   | 283        | 8               |
| Accessories | Â£10,854.15   | 166        | 8               |
| Footwear    | Â£8,296.07    | 143        | 8               |

**Insight:** Apparel and clothing drive **65% of revenue** â€” clear signal for inventory investment and marketing focus.

### Location Performance

| Location    | Channel  | Revenue    | Transactions |
|-------------|----------|-----------|--------------|
| Online      | Shopify  | Â£23,291.46| 100          |
| London      | In-store | Â£7,028.92 | 21           |
| Edinburgh   | In-store | Â£6,875.79 | 21           |

**Insight:** While online dominates total revenue, London and Edinburgh stores show **strong per-transaction performance**.

---

## ğŸ—ï¸ Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   POS System    â”‚
â”‚   (In-store)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ CSV Export
         â”‚ (messy data)
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                             â”‚
â”‚         Python + pandas ETL Pipeline        â”‚
â”‚                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  1. Extract                         â”‚   â”‚
â”‚  â”‚     â€¢ Read CSVs                     â”‚   â”‚
â”‚  â”‚     â€¢ Handle encoding issues        â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                  â”‚                          â”‚
â”‚                  â–¼                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  2. Transform                       â”‚   â”‚
â”‚  â”‚     â€¢ Standardize column names      â”‚   â”‚
â”‚  â”‚     â€¢ Custom date parsing           â”‚   â”‚
â”‚  â”‚     â€¢ Remove duplicates             â”‚   â”‚
â”‚  â”‚     â€¢ Clean categories              â”‚   â”‚
â”‚  â”‚     â€¢ Handle missing values         â”‚   â”‚
â”‚  â”‚     â€¢ Add calculated columns        â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                  â”‚                          â”‚
â”‚                  â–¼                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  3. Load                            â”‚   â”‚
â”‚  â”‚     â€¢ Save to SQLite database       â”‚   â”‚
â”‚  â”‚     â€¢ Create indexes                â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ SQLite Databaseâ”‚
         â”‚  sales_clean   â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                 â”‚
         â–¼                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Power BI   â”‚   â”‚  Excel/SQL  â”‚
â”‚  Analysts   â”‚   â”‚   Queries   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Shopify Store   â”‚
â”‚   (Online)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ CSV Export
         â”‚ (messy data)
         â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ’¡ Core Concepts Demonstrated

This project showcases real-world data engineering skills:

- âœ… **ETL fundamentals** (Extract â†’ Transform â†’ Load)
- âœ… **Handling real-world data messiness** (inconsistent schemas, mixed date formats, missing values, duplicates)
- âœ… **Defensive programming** & custom parsing logic
- âœ… **Data quality checks** and validation
- âœ… **Preparing data for analysts** / BI tools
- âœ… **Database design** for analytics workloads
- âœ… **Version control** with Git

---

## ğŸ› ï¸ Tech Stack

- **Python 3.13** â€” Core programming language
- **pandas** â€” Data manipulation and transformation
- **Jupyter Notebook** â€” Interactive development
- **SQLite** â€” Lightweight local database
- **Git & GitHub** â€” Version control and portfolio hosting

---

## ğŸ“ˆ Business Questions Answered

The pipeline enables analysts to answer questions like:

### 1. Revenue & Transaction Count by Channel
```sql
SELECT 
    channel,
    ROUND(SUM(revenue), 2) as total_revenue,
    COUNT(*) as transaction_count,
    ROUND(AVG(revenue), 2) as avg_revenue_per_transaction
FROM sales_clean
GROUP BY channel;
```
**Result:** In-store has higher average transaction value (Â£304 vs Â£233 online)

### 2. Top-Selling Categories by Revenue
```sql
SELECT 
    category,
    ROUND(SUM(revenue), 2) as total_revenue,
    SUM(quantity) as total_units_sold
FROM sales_clean
GROUP BY category
ORDER BY total_revenue DESC;
```
**Result:** Apparel and clothing dominate revenue

### 3. Daily Sales Trend by Channel
```sql
SELECT 
    date,
    channel,
    ROUND(SUM(revenue), 2) as daily_revenue
FROM sales_clean
GROUP BY date, channel
ORDER BY date;
```
**Result:** Clear visibility into daily fluctuations

### 4. Performance by Store/Location
```sql
SELECT 
    location,
    channel,
    ROUND(SUM(revenue), 2) as total_revenue,
    COUNT(*) as transactions
FROM sales_clean
GROUP BY location, channel
ORDER BY total_revenue DESC;
```
**Result:** Online dominates overall, but London & Edinburgh stores perform strongly

---

## ğŸš€ How to Run

### Prerequisites
```bash
python 3.13+
pip install pandas jupyter
```

### Steps

1. **Clone the repository**
   ```bash
   git clone https://github.com/yourusername/uk-retail-etl-pipeline.git
   cd uk-retail-etl-pipeline
   ```

2. **Open the Jupyter notebook**
   ```bash
   jupyter notebook day1_retail_etl.ipynb
   ```

3. **Run all cells**
   - Data generation â†’ Cleaning â†’ Database load â†’ Queries

4. **Query the database**
   ```python
   import sqlite3
   import pandas as pd
   
   conn = sqlite3.connect('retail_sales_day1.db')
   df = pd.read_sql("SELECT * FROM sales_clean LIMIT 10", conn)
   print(df)
   ```

---

## ğŸ“ Project Structure

```
uk-retail-etl-pipeline/
â”œâ”€â”€ README.md                      # This file
â”œâ”€â”€ SETUP.md                       # Detailed setup guide
â”œâ”€â”€ day1_retail_etl.ipynb         # Main ETL pipeline notebook
â”œâ”€â”€ retail_sales_day1.db          # SQLite database (output)
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ pos_sales_messy.csv       # Raw POS data
â”‚   â””â”€â”€ online_sales_messy.csv    # Raw Shopify data
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ architecture_diagram.png  # Visual architecture
â”œâ”€â”€ .gitignore                    # Git ignore rules
â””â”€â”€ LICENSE                       # MIT License
```

---

## ğŸ“ Key Learnings

### The Date Parsing Challenge

**Problem:** Standard pandas date parsing failed on 95%+ of records due to mixed formats:
- `2026-01-21` (ISO format)
- `01-21-2026` (US format)
- `21/01/2026` (UK format)

**Solution:** Built custom parsing logic that tries each format sequentially:

```python
def parse_flexible_date(date_str):
    formats = ['%Y-%m-%d', '%m-%d-%Y', '%d/%m/%Y']
    for fmt in formats:
        try:
            return pd.to_datetime(date_str, format=fmt)
        except:
            continue
    return None
```

**Result:** 0% failure rate, accurate date dimensions for time-series analysis

### Other Insights

- Real data is **always messy** â€” especially dates from different systems
- Automatic parsing often fails â€” **custom logic frequently required**
- Preparing data for analysts (clean schema + date dimensions) is **core data engineering work**
- Business context matters â€” understanding retail KPIs shaped the transformation logic

---

## ğŸ”® Future Enhancements

- [ ] Implement incremental loading for production deployment
- [ ] Add data quality monitoring and alerting
- [ ] Create automated daily reporting dashboards
- [ ] Add schema validation and error logging
- [ ] Expand to handle additional data sources (returns, inventory)
- [ ] Implement data lineage tracking
- [ ] Add unit tests for transformation logic

---

## ğŸ“ Interview Talking Points

> *"This was a realistic UK retail data problem â€” messy multi-source sales files. I built an ETL pipeline that cleaned inconsistent columns, fixed mixed date formats with custom logic, removed duplicates, and loaded the result into SQLite. I then wrote SQL queries that let the business answer real questions like channel performance and top categories â€” all things that were impossible with the raw data."*

---

## ğŸ“« Contact

**Rakesh Mohankumar**  
MSc Business Analytics @ University of Exeter  
Seeking UK Data/Analytics Roles

- ğŸ“§ Email: [rakesh.tiya@gmail.com]
- ğŸ’¼ LinkedIn: [https://www.linkedin.com/in/rakesh-mohankumar-367915161/]
- ğŸ™ GitHub: [https://github.com/rakeshtiya/rakeshtiya]


---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

This project is part of a progressive data engineering portfolio series demonstrating production-ready skills for UK analytics roles.

**Part of:** Data Engineering Portfolio Series  
**Project:** Day 1 of 30  
**Focus:** ETL Fundamentals & Data Quality

---

**â­ If you found this project helpful, please consider giving it a star!**

*Built with â¤ï¸ for demonstrating real-world data engineering skills*
